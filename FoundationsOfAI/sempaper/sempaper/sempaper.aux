\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{Markelic04reinforcementlearning}
\citation{Sutton98}
\citation{Barto90learningand}
\citation{Howard60}
\citation{Littman94}
\citation{Sutton98}
\citation{Sutton98}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:intro}{{1}{2}{Introduction\relax }{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reinforcement learning in general}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The main idea behind reinforcement learning}{2}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The model of the agent and its environment for reinforcement learning (taken from \cite  {Sutton98})}}{3}{figure.1}}
\newlabel{agentWorldModel}{{1}{3}{The model of the agent and its environment for reinforcement learning (taken from \cite {Sutton98})\relax }{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Important problems}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {paragraph}{The control problem}{3}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Problem of exploration and exploitation}{3}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{The prediction problem}{3}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Partial observability problem}{3}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Curse of Dimensionality}{3}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Credit Structuring Problem}{3}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Value function and Policies}{4}{subsection.2.3}}
\citation{Sutton98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Separation from other machine learning approaches}{5}{subsection.2.4}}
\citation{Howard60}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}The Markov Property and Markov Decision Processes}{6}{subsection.2.5}}
\citation{Sutton98}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The principle of policy iteration}}{7}{figure.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}How to find optimal policies?}{7}{subsection.2.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Dynamic Programming}{7}{subsubsection.2.6.1}}
\@writefile{toc}{\contentsline {paragraph}{Value iteration}{7}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Policy Iteration}{7}{section*.9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Policy Iteration}}{8}{algorithm.1}}
\newlabel{alg1}{{1}{8}{Policy Iteration\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Monte Carlo Methods}{8}{subsubsection.2.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Initial situation of the grid world with rewards for the actions}}{9}{figure.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces situation after one value update}}{9}{figure.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces situation after all value updates}}{9}{figure.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces situation after policy change}}{9}{figure.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces algorithm terminates after final value update}}{9}{figure.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Temporal Difference Learning}{9}{subsubsection.2.6.3}}
\@writefile{toc}{\contentsline {paragraph}{The Q-Learning algorithm}{9}{section*.10}}
\citation{Sutton98}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Monte Carlo Algorithm for finding an optimal policy}}{10}{algorithm.2}}
\newlabel{monteCarlo}{{2}{10}{Monte Carlo Methods\relax }{algorithm.2}{}}
\newlabel{qlearning}{{2.6.3}{10}{The Q-Learning algorithm\relax }{section*.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Q-Learning algorithm}}{10}{algorithm.3}}
\@writefile{toc}{\contentsline {paragraph}{Sarsa}{10}{section*.11}}
\citation{Littman94}
\citation{owen95}
\@writefile{toc}{\contentsline {paragraph}{TD($\lambda $)}{11}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Eigibility traces as an improvement of reinforcement learning algorithms}{11}{subsubsection.2.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Applications of reinforcement learning}{11}{subsection.2.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}TD-Gammon}{11}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Multi-agent reinforcement learning}{11}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The framework of Markov Games}{11}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Finding optimal policies in Markov games}{12}{subsubsection.3.1.1}}
\citation{Littman94}
\citation{Litman94}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces The Minimax-Q algorithm (adapted from \cite  {Litman94})}}{13}{algorithm.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Minimax-Q learning algorithm}{13}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Soccer as an application of the Minimax-Q algorithm}{13}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Game description and rules}{13}{subsubsection.3.3.1}}
\citation{Littman94}
\citation{Littman94}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Initial situation}}{14}{figure.8}}
\newlabel{initialSetup}{{8}{14}{Initial situation\relax }{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Situation that requires a probabilistic policy}}{14}{figure.9}}
\newlabel{probPolicyRequired}{{9}{14}{Situation that requires a probabilistic policy\relax }{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Training and Testing}{14}{subsubsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Results}{14}{subsubsection.3.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Related Work}{14}{section.4}}
\newlabel{sec:relatedwork}{{4}{14}{Related Work\relax }{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Outlook}{14}{section.5}}
\newlabel{sec:conclusion_outlook}{{5}{14}{Conclusion and Outlook\relax }{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results of games of the agents trained with different policies (taken from \cite  {Littman94})}}{15}{table.1}}
\citation{*}
\bibstyle{alpha}
\bibdata{sempaper}
\bibcite{Arai00multi-agentreinforcement}{ASP00}
\bibcite{Barto90learningand}{BSW90}
\bibcite{Crites96improvingelevator}{CB96}
\bibcite{Howard60}{How60}
\bibcite{Littman94}{Lit94}
\bibcite{Littman01}{Lit01}
\bibcite{Markelic04reinforcementlearning}{Mar04}
\bibcite{owen95}{Owe95}
\bibcite{Samuel59}{Sam59}
\bibcite{Sutton98}{SB98}
\bibcite{Tan93multi-agentreinforcement}{Tan93}
\bibcite{Tuyls05multi-agentrelational}{TCR{$^{+}$}05}
\bibcite{Tesauro92practicalissues}{Tes92}
\bibcite{Tesauro02programmingbackgammon}{Tes02}
