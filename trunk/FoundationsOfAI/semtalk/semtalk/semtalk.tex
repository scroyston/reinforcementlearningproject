%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Mode
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mode<presentation>
{
  %\usetheme[hideallsubsections]{Aachen}
  %\usetheme{Luebeck}
  \usetheme{Malmoe}
  \usefonttheme{professionalfonts}
  \usecolortheme{orchid}
  % oder ...

  \setbeamercovered{transparent}
  % oder auch nicht
}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Packages
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[latin1]{inputenc}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{xcolor}
%\usefonttheme[onlymath]{serif}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{times}
%\usepackage[T1]{fontenc}
\usepackage{listings}
%\usepackage{psfrag}
%\usepackage{rotating}
%\usepackage{xmpmulti}
%\usepackage{pgfarrows}
%\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{algorithmic}
%\usepackage{program}
%\usepackage[pst-all]{graphicx}


%%%%%%%%%%%%%
\newcommand{\topalign}[1]{%
  \begin{columns}%
    \begin{column}[T]{0pt}%
      \vspace*{\textheight}%
    \end{column}%
    \begin{column}[T]{\textwidth}%
      #1%
    \end{column}%
  \end{columns}%
}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Macros/Defs
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\colorlet{lightblue}{blue!50!white}

\def\blue#1{\color{blue}#1\color{black}}
\def\lightblue#1{\color{lightblue}#1\color{black}}
%\def\lightblue#1{\color{blue}\begin{colormixin}{10!white}#1\end{colormixin}\color{black}}
\def\red#1{\color{red}#1\color{black}}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Title Information
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Multi-Agent Reinforcement Learning]{Multi-Agent Reinforcement Learning}

\author[C.Kalla]{Christian Kalla}
\institute[RWTH Aachen University]
{ Knowledge-Based Systems Group\\
  RWTH Aachen University
}

\date[8.6.2009] % (optional, sollte der abgekürzte Konferenzname sein)
{8.6.2009/Seminar- Foundations of AI}

\subject{Subject}

\pgfdeclareimage[height=0.5cm]{university-logo}{pics/logo_blau_rechts}
\logo{\pgfuseimage{university-logo}}


\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% make the title area


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Begin Document
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \titlepage
\end{frame}

\section{Introduction}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Idea of Reinforcement Learning}
  \begin{itemize}
  \item learning by interacting with the environment
  \item taking actions and receiving rewards
  \item trying to maximize long-term reward
  \item mathematical description: \textcolor{red}{Markov Decision Process}
  \end{itemize}
\end{frame}
\begin{frame}
\frametitle{Comparison to other Machine Learning Approaches}
\begin{itemize}
\item \textbf{Supervised Learning}
\pause
\begin{itemize}
\item given correct input-output pairs
\item correct classification of data given $\rightarrow$ \textcolor{red}{"teacher"}
\item example: digit recognition
\end{itemize}
\pause
\item \textbf{Unsupervised Learning}
\begin{itemize}
\item just "raw data" without labeling given
\item no "teacher"
\item example: clustering methods (k-means,...)
\end{itemize}
\pause
\item \textbf{Reinforcement Learning}
\begin{itemize}
\item learning by interacting with the environment
\item "natural" approach (related to human learning)
\item feedback from environment in terms of rewards
\item goal: maximize long-term reward
\end{itemize}
\end{itemize}
\end{frame}         

\section{Foundations of Reinforcement Learning}
\subsection{Markov Decision processes}
\begin{frame}
\frametitle{The Markov Property}
\begin{block}{Markov Property}
\begin{equation*}
\begin{align}
&Pr\left\{s_{t+1}=s',r_{t+1}=r| s_{t},a_{t},r_{t},s_{t-1},a_{t-1},...,r_{1},s_{0},a_{0} \right\}\\
=&Pr\left\{s_{t+1}=s',r_{t+1}=r| s_{t},a_{t} \right\}   
\end{align}
\end{equation}
\end{block}
The probability distribution of the next state only depends on the previous state and not on all the states visited before
\end{frame}
\begin{frame}
\frametitle{Markov Decision Process(MDP)}
\begin{block}{Components of an MDP}
\begin{itemize}
\item a set of states $\mathcal{S}$
\item a set of actions $\mathcal{A}$
\item a set of rewards $\mathfrak{R}$
\item a transition function $T:\mathcal{S}\times \mathcal{A}\rightarrow PD(\mathcal{S})$ where $PD(\mathcal{S})$ denotes the set of probability distribution over $\mathcal{S}$
\item a reward function $R:\mathcal{S} \times \mathcal{A} \rightarrow \mathfrak{R}$
\end{itemize}
\end{block}
\begin{alertblock}{Goal:Maximize expected sum of discounted future rewards:}
\begin{equation}
E \left\{\sum_{j=0}^{\infty}{\gamma^{j}r_{t+j}} \right\}
\end{equation}
\end{alertblock}
\end{frame}
\subsection{Policies}
\subsection{Value- and State-Value-Function}
\subsection{Basic algorithms}
\begin{frame}
\frametitle{Categorization of Algorithms}
\end{frame}
\begin{frame}
\frametitle{The Q-Learning Algorithm}
\begin{block}{Q-Learning Algorithm}
\begin{algorithm}
\begin{algorithmic}
\STATE Initialize $Q(s,a)$ arbitrarily
\FOR{each episode}
\STATE Initialize $s$
\REPEAT
\STATE Choose a from s using policy derived from $Q$
\STATE Take action a and observe $a,s'$
\STATE $Q(s,a) \gets Q(s,a)+\alpha(r+\gamma \max_{a'} Q(s',a') - Q(s,a))$
\STATE $s \gets s'$
\UNTIL{s is terminal}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{block}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Yet Another Slide}
  \begin{columns}
    \begin{column}{0.4\textwidth}
      1st column
      \begin{itemize}
      \item Item1
      \item Item2
      \item ...
      \end{itemize}
    \end{column}
    \hfill
    \begin{column}{0.6\textwidth}
      2nd column
      \begin{center}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}
 
\section{Multi-Agent Reinforcement Learning}
\subsection{General Problems}
\begin{frame}
\frametitle{Overview of MARL algorithms}
\includegraphics[viewport=312 416 562 615,keepaspectratio,clip,page=1]{../../survey1.pdf}
\end{frame}
\subsection{Sharing Knowledge}
\subsection{Game-theoretic Approaches}
\begin{frame}
\frametitle{A Robot Soccer Scenario}
\end{frame}
\begin{frame}
\frametitle{The Minimax-Q Learning Algorithm}
\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Last Slide}
  \href{http://www.cs.ubc.ca/~poole/demos/rl/q.html}{\beamergotobutton{Q Learning applet}}  
\end{frame}



%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  End Document
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
