@INPROCEEDINGS{Littman94,
    author = {Michael L. Littman},
    title = {Markov games as a framework for multi-agent reinforcement learning},
    booktitle = {In Proceedings of the Eleventh International Conference on Machine Learning},
    year = {1994},
    pages = {157--163},
    publisher = {Morgan Kaufmann}
}
@book{owen95,
author={Guillermo Owen},
title={Game Theory: Third Edition},
publisher={Academic Press},
year=1995
}

@book{Sutton98,
	abstract = {{Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In <i>Reinforcement Learning</i>, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.<br /> <br /> The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.}},
	author = {Sutton, Richard  S.  and Barto, Andrew  G. },
	citeulike-article-id = {112017},
	howpublished = {Hardcover},
	isbn = {0262193981},
	keywords = {introduction, reinforcement\_learning, survey},
	month = {March},
	posted-at = {2008-05-01 10:21:09},
	priority = {3},
	publisher = {{The MIT Press}},
	title = {Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262193981},
	year = {1998}
}
@ARTICLE{backgammon,
    author = {Gerald Tesauro},
    title = {Programming backgammon using self-teaching neural nets},
    journal = {Artificial Intelligence},
    year = {2002},
    volume = {134},
    pages = {181--199}
}
@INPROCEEDINGS{Tesauro92,
    author = {Gerald Tesauro},
    title = {Practical issues in temporal difference learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {257--277}
}
@INPROCEEDINGS{Tuyls05multi-agentrelational,
    author = {Karl Tuyls and Tom Croonenborghs and Jan Ramon and Robby Goetschalckx and Maurice Bruynooghe},
    title = {Multi-agent relational reinforcement learning},
    booktitle = {Proceedings of the First International Workshop on Learning and Adaptation in Multi Agent Systems},
    year = {2005},
    pages = {123--132}
}
@INPROCEEDINGS{Tan93multi-agentreinforcement,
    author = {Ming Tan},
    title = {Multi-agent reinforcement learning: Independent vs. cooperative agents},
    booktitle = {In Proceedings of the Tenth International Conference on Machine Learning},
    year = {1993},
    pages = {330--337},
    publisher = {Morgan Kaufmann}
}
@INPROCEEDINGS{Arai00multi-agentreinforcement,
    author = {Sachiyo Arai and Katia Sycara and Terry R. Payne},
    title = {Multi-agent reinforcement learning for planning and scheduling multiple goals},
    booktitle = {In Proceedings of the Fourth International Conference on MultiAgent Systems},
    year = {2000},
    pages = {359--360}
}
@TECHREPORT{Markelic,
	author={Markelic,Irene},
	title={{Reinforcement Learning als Methode zur Entscheidungsfindung beim simulierten Roboterfu\ss ball}},
	institution={Universit\"at Koblenz-Landau},
	year={2004},
	url={http://www.uni-koblenz.de/~fruit/ftp/teaching/mar04-studienarbeit.pdf}
	
	}
@INPROCEEDINGS{Barto90learningand,
    author = {Andrew G. Barto and R. S. Sutton and C. J. C. H. Watkins},
    title = {Learning and sequential decision making},
    booktitle = {Learning and Computational Neuroscience},
    year = {1990},
    pages = {539--602},
    publisher = {MIT Press}
}
@BOOK{Howard60,
	author={Howard,Ronald A.},
	title={Dynamic Programming and Markov Processes},
	year={1960},
	publisher={{The MIT Press}},
	address={Cambridge, Massachusetts}
	}
@ARTICLE{Samuel59,
author = "A. L. Samuel",
title = "Some studies in machine learning using the game of checkers",
journal = "IBM J. of Res. Develop.",
volume = "3",
month = jul,
year = "1959",
pages = "211--229",
note = "(Reprinted in {\em Computers and Thought}, (eds. E. A. Feigenbaum and J. Feldman), McGraw-Hill, 1963, pages 39--70).",
}
@article{Littman01,
title = "Value-function reinforcement learning in Markov games",
journal = "Cognitive Systems Research",
volume = "2",
number = "1",
pages = "55 - 66",
year = "2001",
note = "",
issn = "1389-0417",
doi = "DOI: 10.1016/S1389-0417(01)00015-8",
url = "http://www.sciencedirect.com/science/article/B6W6C-430G1TK-4/2/822caf1574be32ae91adf15de90becc4",
author = "Michael L. Littman",
keywords = "Reinforcement learning",
keywords = "Temporal difference learning",
keywords = "Value functions",
keywords = "Game theory",
keywords = "Markov games",
keywords = "Q-learning",
keywords = "Nash equilibria"
}
@MISC{learningContest,
title="http://2009.rl-competition.org",
url="http://2009.rl-competition.org"
}
@inproceedings{survey1,
   author={L. Bu\c{s}oniu and R. Babu\v{s}ka and B. {De Schutter}},
   title={Multi-agent reinforcement learning: A survey},
   booktitle={Proceedings of the 9th International Conference on Control, Automation, Robotics and Vision (ICARCV 2006)},
   address={Singapore},
   pages={527--532},
   month=dec,
   year={2006}
}
@article{survey2,
   author={L. Bu{\c{s}}oniu and R. Babu{\v{s}}ka and B. {D}e Schutter},
   title={A comprehensive survey of multi-agent reinforcement learning},
   journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews},
   volume={38},
   number={2},
   pages={156--172},
   month=mar,
   year={2008},
   doi={10.1109/TSMCC.2007.913919}
}
@inproceedings{distributedq,
  author    = {Martin Lauer and
               Martin A. Riedmiller},
  title     = {An Algorithm for Distributed Reinforcement Learning in Cooperative
               Multi-Agent Systems},
  booktitle = {ICML},
  year      = {2000},
  pages     = {535-542},
  crossref  = {DBLP:conf/icml/2000},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
@Article{nashq,
  author =	 {Junling Hu and Michael P. Wellman},
  title =	 {Nash Q-Learning for General-Sum Stochastic Games},
  journal =	 {Journal of Machine Learning Research},
  year =	 2003,
  volume =	 4,
  pages =	 {1039--1069},
  abstract =	 {We extend Q-learning to a noncooperative multiagent
                  context, using the framework of generalsum
                  stochastic games. A learning agent maintains
                  Q-functions over joint actions, and performs updates
                  based on assuming Nash equilibrium behavior over the
                  current Q-values. This learning protocol provably
                  converges given certain restrictions on the stage
                  games (defined by Q-values) that arise during
                  learning. Experiments with a pair of two-player grid
                  games suggest that such restrictions on the game
                  structure are not necessarily required. Stage games
                  encountered during learning in both grid
                  environments violate the conditions. However,
                  learning consistently converges in the first grid
                  game, which has a unique equilibrium Q-function, but
                  sometimes fails to converge in the second, which has
                  three different equilibrium Q-functions. In a
                  comparison of offline learning performance in both
                  games, we find agents are more likely to reach a
                  joint optimal path with Nash Q-learning than with a
                  single-agent Q-learning method. When at least one
                  agent adopts Nash Q-learning, the performance of
                  both agents is better than using single-agent
                  Q-learning. We have also implemented an online
                  version of Nash Q-learning that balances exploration
                  with exploitation, yielding improved performance.},
  keywords =     {multiagent reinforcement learning},
  googleid = 	 {GF9Y4c6NGIEJ:scholar.google.com/},
  url = 	 {http://jmvidal.cse.sc.edu/library/hu03a.pdf},
  cluster = 	 {9302340950017203992}
}
@INPROCEEDINGS{singletomulti,
    author = {Sandip Sen and Ip Sen and Mahendra Sekaran and John Hale},
    title = {Learning to Coordinate Without Sharing Information},
    booktitle = {In Proceedings of the Twelfth National Conference on Artificial Intelligence},
    year = {1994},
    pages = {426--431}
}
@inproceedings{coordination,
 author = {Boutilier, Craig},
 title = {Planning, learning and coordination in multiagent decision processes},
 booktitle = {TARK '96: Proceedings of the 6th conference on Theoretical aspects of rationality and knowledge},
 year = {1996},
 isbn = {1-55860-417-9},
 pages = {195--210},
 location = {The Netherlands},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
 }
 @incollection{opponentModel,
  Author =      {David Carmel and Shaul Markovitch},
  Title =       {Opponent Modeling in Multi-agent Systems},
  Year =        {1996},
  Booktitle =   {Adaption And Learning In Multi-Agent Systems},
  Volume =      {1042},
  Publisher =   {Springer-Verlag},
  Keywords =    {Opponent Modeling, Multi-Agent Systems, Learning in Games},
  Editor =      {Gerhard Weiss and Sandip Sen},
  Secondary-keywords =  {Repeated Games, Learning DFA},
  Series =      {Lecture Notes in Artificial Intelligence},
  Url =         {http://www.cs.technion.ac.il/~shaulm/papers/pdf/Carmel-Markovitch-lnai1996.pdf},
  Abstract =    {Agents that operate in a multi-agent system need an efficient strategy to handle
             their encounters with other agents involved. Searching for an optimal
             interactive strategy is a hard problem because it depends mostly on the
             behavior of the others. In this work, interaction among agents is represented
             as a repeated two-player game, where the agents' objective is to look for a
             strategy that maximizes their expected sum of rewards in the game. We assume
             that agents' strategies can be modeled as finite automata. A model-based
             approach is presented as a possible method for learning an effective
             interactive strategy. First, we describe how an agent should find an optimal
             strategy against a given model. Second, we present a heuristic algorithm that
             infers a model of the opponent's automaton from its input/output behavior. A
             set of experiments that show the potential merit of the algorithm is reported
             as well.}
}
@ARTICLE{Mataric97,
    author = {Maja J. Mataric},
    title = {Reinforcement Learning in the Multi-Robot Domain},
    journal = {Autonomous Robots},
    year = {1997},
    volume = {4},
    pages = {73--83}
}
@article{singletomulti2,
 author = {Tuyls, Karl and Hoen, Pieter Jan and Vanschoenwinkel, Bram},
 title = {An Evolutionary Dynamical Analysis of Multi-Agent Learning in Iterated Games},
 journal = {Autonomous Agents and Multi-Agent Systems},
 volume = {12},
 number = {1},
 year = {2006},
 issn = {1387-2532},
 pages = {115--153},
 doi = {http://dx.doi.org/10.1007/s10458-005-3783-9},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 }
 @ARTICLE{wolfphc,
    author = {Michael Bowling and Manuela Veloso},
    title = {Multiagent Learning Using a Variable Learning Rate},
    journal = {Artificial Intelligence},
    year = {2002},
    volume = {136},
    pages = {215--250}
}
@inproceedings{nscp,
 author = {Weinberg, Michael and Rosenschein, Jeffrey S.},
 title = {Best-Response Multiagent Learning in Non-Stationary Environments},
 booktitle = {AAMAS '04: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems},
 year = {2004},
 isbn = {1-58113-864-4},
 pages = {506--513},
 location = {New York, New York},
 doi = {http://dx.doi.org/10.1109/AAMAS.2004.75},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 }
 @ARTICLE{pomdp,
    author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
    title = {Planning and Acting in Partially Observable Stochastic Domains},
    journal = {Artificial Intelligence},
    year = {1995},
    volume = {101},
    pages = {99--134}
}
@article{automatedtrading,
 author = {Tesauro, Gerald and Kephart, Jeffrey O.},
 title = {Pricing in Agent Economies Using Multi-Agent Q-Learning},
 journal = {Autonomous Agents and Multi-Agent Systems},
 volume = {5},
 number = {3},
 year = {2002},
 issn = {1387-2532},
 pages = {289--304},
 doi = {http://dx.doi.org/10.1023/A:1015504423309},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 }
 @inproceedings{distributedcontrol,
 author = {Wiering, Marco},
 title = {Multi-Agent Reinforcement Leraning for Traffic Light Control},
 booktitle = {ICML '00: Proceedings of the Seventeenth International Conference on Machine Learning},
 year = {2000},
 isbn = {1-55860-707-2},
 pages = {1151--1158},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
 }
 @INPROCEEDINGS{robotsoccer,
    author = {M. Riedmiller and A. Merke and D. Meier and A. Hoffmann and A. Sinner and O. Thate and R. Ehrmann},
    title = {Karlsruhe Brainstormers - A Reinforcement Learning approach to robotic soccer},
    booktitle = {RoboCup-2000: Robot Soccer World Cup IV, LNCS},
    year = {},
    pages = {367--372},
    publisher = {Springer}
}
@INPROCEEDINGS{elevator1,
    author = {Robert H. Crites and Andrew G. Barto},
    title = {Improving elevator performance using reinforcement learning},
    booktitle = {Advances in Neural Information Processing Systems 8},
    year = {1996},
    pages = {1017--1023},
    publisher = {MIT Press}
}
@article{elevator2,
 author = {Crites, Robert H. and Barto, Andrew G.},
 title = {Elevator Group Control Using Multiple Reinforcement Learning Agents},
 journal = {Mach. Learn.},
 volume = {33},
 number = {2-3},
 year = {1998},
 issn = {0885-6125},
 pages = {235--262},
 doi = {http://dx.doi.org/10.1023/A:1007518724497},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 }








